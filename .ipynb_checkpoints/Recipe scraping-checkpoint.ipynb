{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe scraping\n",
    "This script scrapes recipetineats.com to find ingredients for every recipe in each cuisine category on the site. It creates csv files listing the ingredients and quantities for each cuisine for use in association rule mining.  \n",
    "\n",
    "The follow up to this script is the \"recipe association rule mining\" script which uses the apriori algorithm to find the most common groupings of ingredients in each cuisine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "# import mechanicalsoup\n",
    "import lxml.html as html\n",
    "import requests\n",
    "import time\n",
    "#browser = mechanicalsoup.Browser()\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set url for home page of cuisine based on user input\n",
    "cuisine = input(\"What cuisine do you want to explore? Choose from Chinese, Korean, Thai, Vietnamese, Indian, Italian, French, Greek, Mediterranean, South-american, Mexican, Middle-eastern. (Include hyphens).\")\n",
    "url = \"https://www.recipetineats.com/category/\" + cuisine + \"-recipes/\"\n",
    "\n",
    "# Set url and parse html\n",
    "page = urlopen(url)\n",
    "html_1 = page.read().decode('utf-8')\n",
    "soup = BeautifulSoup(html_1, 'html.parser')\n",
    "\n",
    "# Create list of urls within cuisine category\n",
    "# This is needed because not all recipes will be found on the home page\n",
    "n = 1\n",
    "all_pages = [url]\n",
    "for n in range(15):\n",
    "# Used range of 15 because no cuisines have more than 15 pages\n",
    "    for link in soup.find_all(\"link\", rel = \"next\"):\n",
    "        all_pages.append(link[\"href\"])\n",
    "        url = all_pages[n+1]\n",
    "        page = urlopen(url)\n",
    "        html_1 = page.read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html_1, 'html.parser')\n",
    "        n = n + 1\n",
    "\n",
    "# Create list of recipe pages\n",
    "recipe_pages = []\n",
    "for recipe in all_pages:        \n",
    "    url = recipe\n",
    "    page = urlopen(url)\n",
    "    html_1 = page.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html_1, 'html.parser')\n",
    "\n",
    "    # Loop through html adding recipe links to 'recipe_pages' variable\n",
    "    for links in soup.find_all(\"a\", rel = \"bookmark\"):\n",
    "        recipe_pages.append(links[\"href\"])\n",
    "\n",
    "\n",
    "# Build list of ingredients, amounts and associated servings\n",
    "recipes_list = []\n",
    "for n in recipe_pages:\n",
    "    page = requests.get(n)\n",
    "    pagehtml = html.fromstring(page.content)\n",
    "    recipe_title = (pagehtml.xpath(\"//title\")[0].text_content().replace(\" | RecipeTin Eats\", \"\"))\n",
    "    try: # This deals with the fact that servings are held within one of two html structures\n",
    "        servings = int(pagehtml.xpath(\"//*[@class='wprm-recipe-servings-with-unit']\")[0][0].text_content())\n",
    "    except IndexError:\n",
    "            try:\n",
    "                servings = int(pagehtml.xpath(\"//div[@class='wprm-entry-servings']\")[0][0][1].text_content())\n",
    "            except IndexError:\n",
    "                pass\n",
    "    pass\n",
    "    recipe_ingredients = [\n",
    "        {\n",
    "            \"meal\": recipe_title,\n",
    "            'servings': servings,\n",
    "            \"name\": i.xpath(\"*[@class='wprm-recipe-ingredient-name']\")[0].text_content()\n",
    "                if i.xpath(\"*[@class='wprm-recipe-ingredient-name']\")\n",
    "                else np.nan, \n",
    "            \"amount\": i.xpath(\"*[@class='wprm-recipe-ingredient-amount']\")[0].text_content()\n",
    "                if i.xpath(\"*[@class='wprm-recipe-ingredient-amount']\")\n",
    "                else np.NAN,\n",
    "            \"unit\": i.xpath(\"*[@class='wprm-recipe-ingredient-unit']\")[0].text_content()\n",
    "                if i.xpath(\"*[@class='wprm-recipe-ingredient-unit']\")\n",
    "                else np.NAN,\n",
    "            \"notes\": i.xpath(\n",
    "                    \"*[@class='wprm-recipe-ingredient-notes wprm-recipe-ingredient-notes-faded']\"\n",
    "                )[0]\n",
    "                .text_content()\n",
    "                .lstrip(\", \")\n",
    "                if i.xpath(\n",
    "                    \"*[@class='wprm-recipe-ingredient-notes wprm-recipe-ingredient-notes-faded']\"\n",
    "                )\n",
    "                else np.NAN,\n",
    "        }\n",
    "        for i in pagehtml.xpath(\"//*[@class='wprm-recipe-ingredients']/*\")\n",
    "        ]\n",
    "    recipes_list.extend(recipe_ingredients)\n",
    "\n",
    "# Create dataframe from list\n",
    "data = pd.DataFrame(recipes_list)\n",
    "\n",
    "# Check number of meals captured (used when manually verifying all pages were scraped)\n",
    "print(f'number of meals scraped: {data.meal.nunique()}')\n",
    "\n",
    "# Rearrange dataframe columns\n",
    "data = data[['meal', 'servings', 'name', 'amount', 'unit', 'notes']]\n",
    "\n",
    "# Export to csv\n",
    "data.to_csv(f\"{cuisine}.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
